(tf-env) C:\Users\gabri\Downloads\Proyecto2>python entrenar_distilbert_es_unificado_V11.py
================================================================================
🎯  ENTRENAMIENTO V11 - REGULARIZACIÓN MÁXIMA (Objetivo: Gap < 0.04) - CORREGIDA
================================================================================
📊 ANÁLISIS V10 → V11:
   V10 Gap observado: ~0.10 🚨 (categoría ALTO)
   V11 Objetivo: < 0.04 ✅ (categoría BUENO)
   Reducción necesaria: ~60% del gap actual

🔧 CORRECCIONES V11:
   ❌ Label smoothing eliminado (no soportado)
   ✅ Noise injection añadido (alternativa)
   ✅ Batch size variable (4,6,8)

🛡️ ESTRATEGIAS V11 MANTENIDAS:
   • Learning rates MÁS BAJAS: 5e-6 a 8e-7
   • Dropout MÁS AGRESIVO: 0.4-0.7
   • L2 regularization MÁS FUERTE: Mínimo 0.05
   • Weight decay manual 2X: 0.02
   • Bias/Activity regularization 2X: 0.2
   • Reducción LR más agresiva: 0.15
   • Paciencia aumentada: 8 épocas
================================================================================
✅ GPU detectada: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
✅ Precisión Mixta: mixed_float16
--- Fase 1: Cargando y procesando el corpus (MODO V11 - REGULARIZACIÓN MÁXIMA CORREGIDA) ---

--- Análisis de Balance del Corpus ---
Total de registros limpios: 61674
Noticias FALSAS (0): 30734 registros (49.8%)
Noticias REALES (1): 30940 registros (50.2%)
-------------------------------------------------

🔄 División V11: 70% entrenamiento, 10% validación, 20% pruebas...
Usando batch_size: 8
Tamaño del set de entrenamiento: 43171 registros (70.0%)
Tamaño del set de validación: 6167 registros (10.0%)
Tamaño del set de pruebas: 12336 registros (20.0%)

Cargando tokenizador para el modelo: 'distilbert-base-multilingual-cased'
C:\Users\gabri\.conda\envs\tf-env\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Tokenizando con MAX_LENGTH=128 (reducido para evitar overfitting)...

--- Fase 2: Optimización V11 (Regularización Máxima CORREGIDA) ---
🎯 OBJETIVO ESPECÍFICO: Gap de pérdida < 0.04
🔍 Hiperparámetros con regularización máxima (sin label_smoothing)
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ V11 CORREGIDA: Usando Adam con regularización MÁXIMA (sin label_smoothing)
🔍 Buscando hiperparámetros V11 (regularización máxima CORREGIDA)...

Search: Running Trial #1

Value             |Best Value So Far |Hyperparameter
2e-06             |2e-06             |learning_rate
0.7               |0.7               |dropout_rate
0.05              |0.05              |l2_regularization
0.03              |0.03              |noise_factor
4                 |4                 |batch_size

C:\Users\gabri\.conda\envs\tf-env\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ V11 CORREGIDA: Usando Adam con regularización MÁXIMA (sin label_smoothing)
Epoch 1/8
5397/5397 [==============================] - ETA: 0s - loss: inf - accuracy: 0.5695
📊 V11 Monitoreo Época 1:
   Gap de pérdida (val - train): -inf
   Gap de exactitud (train - val): -0.0964
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 747s 135ms/step - loss: inf - accuracy: 0.5695 - val_loss: 0.7011 - val_accuracy: 0.6660
Epoch 2/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7149 - accuracy: 0.5936
📊 V11 Monitoreo Época 2:
   Gap de pérdida (val - train): -0.0213
   Gap de exactitud (train - val): -0.0457
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 737s 136ms/step - loss: 0.7149 - accuracy: 0.5936 - val_loss: 0.6936 - val_accuracy: 0.6394
Epoch 3/8
5397/5397 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.7758
📊 V11 Monitoreo Época 3:
   Gap de pérdida (val - train): -0.1601
   Gap de exactitud (train - val): -0.0935
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 734s 136ms/step - loss: 0.6256 - accuracy: 0.7758 - val_loss: 0.4655 - val_accuracy: 0.8693
Epoch 4/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.8991
📊 V11 Monitoreo Época 4:
   Gap de pérdida (val - train): -0.0534
   Gap de exactitud (train - val): -0.0164
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 732s 136ms/step - loss: 0.4100 - accuracy: 0.8991 - val_loss: 0.3566 - val_accuracy: 0.9155
Epoch 5/8
5397/5397 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.9231
📊 V11 Monitoreo Época 5:
   Gap de pérdida (val - train): -0.0338
   Gap de exactitud (train - val): -0.0087
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 734s 136ms/step - loss: 0.3450 - accuracy: 0.9231 - val_loss: 0.3112 - val_accuracy: 0.9317
Epoch 6/8
5397/5397 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.9327
📊 V11 Monitoreo Época 6:
   Gap de pérdida (val - train): -0.0170
   Gap de exactitud (train - val): -0.0005
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 733s 136ms/step - loss: 0.3090 - accuracy: 0.9327 - val_loss: 0.2920 - val_accuracy: 0.9332
Epoch 7/8
5397/5397 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9411
📊 V11 Monitoreo Época 7:
   Gap de pérdida (val - train): -0.0063
   Gap de exactitud (train - val): 0.0036
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 743s 138ms/step - loss: 0.2811 - accuracy: 0.9411 - val_loss: 0.2747 - val_accuracy: 0.9376
Epoch 8/8
5397/5397 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9482
📊 V11 Monitoreo Época 8:
   Gap de pérdida (val - train): -0.0028
   Gap de exactitud (train - val): 0.0067
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 1656s 307ms/step - loss: 0.2598 - accuracy: 0.9482 - val_loss: 0.2570 - val_accuracy: 0.9415

Trial 1 Complete [01h 53m 39s]
val_loss: 0.2570400834083557

Best val_loss So Far: 0.2570400834083557
Total elapsed time: 01h 53m 39s

Search: Running Trial #2

Value             |Best Value So Far |Hyperparameter
1e-06             |2e-06             |learning_rate
0.4               |0.7               |dropout_rate
0.05              |0.05              |l2_regularization
0.02              |0.03              |noise_factor
6                 |4                 |batch_size

Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ V11 CORREGIDA: Usando Adam con regularización MÁXIMA (sin label_smoothing)
Epoch 1/8
5397/5397 [==============================] - ETA: 0s - loss: 5.5939 - accuracy: 0.5341
📊 V11 Monitoreo Época 1:
   Gap de pérdida (val - train): -4.8751
   Gap de exactitud (train - val): -0.0195
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 1173s 211ms/step - loss: 5.5939 - accuracy: 0.5341 - val_loss: 0.7188 - val_accuracy: 0.5536
Epoch 2/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7413 - accuracy: 0.5743
📊 V11 Monitoreo Época 2:
   Gap de pérdida (val - train): -0.0457
   Gap de exactitud (train - val): 0.0716
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🚨 V11 ALERTA: Gap exactitud > 0.04
5397/5397 [==============================] - 753s 139ms/step - loss: 0.7413 - accuracy: 0.5743 - val_loss: 0.6956 - val_accuracy: 0.5027
Epoch 3/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.6019
📊 V11 Monitoreo Época 3:
   Gap de pérdida (val - train): -0.0142
   Gap de exactitud (train - val): -0.0731
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 747s 138ms/step - loss: 0.7071 - accuracy: 0.6019 - val_loss: 0.6929 - val_accuracy: 0.6750
Epoch 4/8
5397/5397 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.7867
📊 V11 Monitoreo Época 4:
   Gap de pérdida (val - train): -0.1000
   Gap de exactitud (train - val): -0.0495
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 741s 137ms/step - loss: 0.6795 - accuracy: 0.7867 - val_loss: 0.5795 - val_accuracy: 0.8362
Epoch 5/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4923 - accuracy: 0.8703
📊 V11 Monitoreo Época 5:
   Gap de pérdida (val - train): -0.0556
   Gap de exactitud (train - val): -0.0110
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 742s 138ms/step - loss: 0.4923 - accuracy: 0.8703 - val_loss: 0.4367 - val_accuracy: 0.8813
Epoch 6/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.8958
📊 V11 Monitoreo Época 6:
   Gap de pérdida (val - train): -0.0351
   Gap de exactitud (train - val): -0.0076
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 743s 138ms/step - loss: 0.4243 - accuracy: 0.8958 - val_loss: 0.3892 - val_accuracy: 0.9034
Epoch 7/8
5397/5397 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.9105
📊 V11 Monitoreo Época 7:
   Gap de pérdida (val - train): -0.0284
   Gap de exactitud (train - val): -0.0040
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 742s 137ms/step - loss: 0.3848 - accuracy: 0.9105 - val_loss: 0.3564 - val_accuracy: 0.9145
Epoch 8/8
5397/5397 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.9186
📊 V11 Monitoreo Época 8:
   Gap de pérdida (val - train): -0.0242
   Gap de exactitud (train - val): -0.0023
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 743s 138ms/step - loss: 0.3563 - accuracy: 0.9186 - val_loss: 0.3321 - val_accuracy: 0.9209

Trial 2 Complete [01h 46m 35s]
val_loss: 0.3320567011833191

Best val_loss So Far: 0.2570400834083557
Total elapsed time: 03h 40m 14s

Search: Running Trial #3

Value             |Best Value So Far |Hyperparameter
8e-07             |2e-06             |learning_rate
0.4               |0.7               |dropout_rate
0.05              |0.05              |l2_regularization
0.02              |0.03              |noise_factor
8                 |4                 |batch_size

Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ V11 CORREGIDA: Usando Adam con regularización MÁXIMA (sin label_smoothing)
Epoch 1/8
5397/5397 [==============================] - ETA: 0s - loss: inf - accuracy: 0.5441
📊 V11 Monitoreo Época 1:
   Gap de pérdida (val - train): -inf
   Gap de exactitud (train - val): -0.0786
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 753s 137ms/step - loss: inf - accuracy: 0.5441 - val_loss: 0.7314 - val_accuracy: 0.6227
Epoch 2/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7591 - accuracy: 0.5741
📊 V11 Monitoreo Época 2:
   Gap de pérdida (val - train): -0.0629
   Gap de exactitud (train - val): -0.0964
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 739s 137ms/step - loss: 0.7591 - accuracy: 0.5741 - val_loss: 0.6961 - val_accuracy: 0.6705
Epoch 3/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7102 - accuracy: 0.5898
📊 V11 Monitoreo Época 3:
   Gap de pérdida (val - train): -0.0169
   Gap de exactitud (train - val): -0.1073
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 741s 137ms/step - loss: 0.7102 - accuracy: 0.5898 - val_loss: 0.6933 - val_accuracy: 0.6971
Epoch 4/8
5397/5397 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.7416
📊 V11 Monitoreo Época 4:
   Gap de pérdida (val - train): -0.0228
   Gap de exactitud (train - val): -0.0823
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 739s 137ms/step - loss: 0.6987 - accuracy: 0.7416 - val_loss: 0.6760 - val_accuracy: 0.8239
Epoch 5/8
5397/5397 [==============================] - ETA: 0s - loss: 0.5765 - accuracy: 0.8353
📊 V11 Monitoreo Época 5:
   Gap de pérdida (val - train): -0.0907
   Gap de exactitud (train - val): -0.0150
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 741s 137ms/step - loss: 0.5765 - accuracy: 0.8353 - val_loss: 0.4858 - val_accuracy: 0.8503
Epoch 6/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.8708
📊 V11 Monitoreo Época 6:
   Gap de pérdida (val - train): -0.0350
   Gap de exactitud (train - val): -0.0065
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 739s 137ms/step - loss: 0.4741 - accuracy: 0.8708 - val_loss: 0.4391 - val_accuracy: 0.8772
Epoch 7/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8913
📊 V11 Monitoreo Época 7:
   Gap de pérdida (val - train): -0.0337
   Gap de exactitud (train - val): -0.0051
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 738s 137ms/step - loss: 0.4298 - accuracy: 0.8913 - val_loss: 0.3961 - val_accuracy: 0.8964
Epoch 8/8
5397/5397 [==============================] - ETA: 0s - loss: 0.3943 - accuracy: 0.9053
📊 V11 Monitoreo Época 8:
   Gap de pérdida (val - train): -0.0229
   Gap de exactitud (train - val): 0.0016
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 743s 138ms/step - loss: 0.3943 - accuracy: 0.9053 - val_loss: 0.3714 - val_accuracy: 0.9037

Trial 3 Complete [01h 38m 57s]
val_loss: 0.3713732957839966

Best val_loss So Far: 0.2570400834083557
Total elapsed time: 05h 19m 10s

Search: Running Trial #4

Value             |Best Value So Far |Hyperparameter
2e-06             |2e-06             |learning_rate
0.6               |0.7               |dropout_rate
0.2               |0.05              |l2_regularization
0.01              |0.03              |noise_factor
6                 |4                 |batch_size

Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ V11 CORREGIDA: Usando Adam con regularización MÁXIMA (sin label_smoothing)
Epoch 1/8
5397/5397 [==============================] - ETA: 0s - loss: 11.5469 - accuracy: 0.5352
📊 V11 Monitoreo Época 1:
   Gap de pérdida (val - train): -10.8206
   Gap de exactitud (train - val): -0.1268
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 755s 138ms/step - loss: 11.5469 - accuracy: 0.5352 - val_loss: 0.7263 - val_accuracy: 0.6621
Epoch 2/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7786 - accuracy: 0.5462
📊 V11 Monitoreo Época 2:
   Gap de pérdida (val - train): -0.0826
   Gap de exactitud (train - val): 0.0435
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🚨 V11 ALERTA: Gap exactitud > 0.04
5397/5397 [==============================] - 738s 137ms/step - loss: 0.7786 - accuracy: 0.5462 - val_loss: 0.6960 - val_accuracy: 0.5027
Epoch 3/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.5034
📊 V11 Monitoreo Época 3:
   Gap de pérdida (val - train): -0.0269
   Gap de exactitud (train - val): 0.0017
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 738s 137ms/step - loss: 0.7211 - accuracy: 0.5034 - val_loss: 0.6942 - val_accuracy: 0.5017
Epoch 4/8
5397/5397 [==============================] - ETA: 0s - loss: 0.7064 - accuracy: 0.5023
📊 V11 Monitoreo Época 4:
   Gap de pérdida (val - train): -0.0124
   Gap de exactitud (train - val): -0.0017
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 740s 137ms/step - loss: 0.7064 - accuracy: 0.5023 - val_loss: 0.6940 - val_accuracy: 0.5040
Epoch 5/8
5397/5397 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.6736
📊 V11 Monitoreo Época 5:
   Gap de pérdida (val - train): -0.0648
   Gap de exactitud (train - val): -0.1928
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 738s 137ms/step - loss: 0.6916 - accuracy: 0.6736 - val_loss: 0.6268 - val_accuracy: 0.8664
Epoch 6/8
5397/5397 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.8868
📊 V11 Monitoreo Época 6:
   Gap de pérdida (val - train): -0.0620
   Gap de exactitud (train - val): -0.0113
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 738s 137ms/step - loss: 0.5617 - accuracy: 0.8868 - val_loss: 0.4997 - val_accuracy: 0.8982
Epoch 7/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.9073
📊 V11 Monitoreo Época 7:
   Gap de pérdida (val - train): -0.0345
   Gap de exactitud (train - val): -0.0009
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 739s 137ms/step - loss: 0.4856 - accuracy: 0.9073 - val_loss: 0.4511 - val_accuracy: 0.9082
Epoch 8/8
5397/5397 [==============================] - ETA: 0s - loss: 0.4370 - accuracy: 0.9254
📊 V11 Monitoreo Época 8:
   Gap de pérdida (val - train): -0.0201
   Gap de exactitud (train - val): 0.0024
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
5397/5397 [==============================] - 739s 137ms/step - loss: 0.4370 - accuracy: 0.9254 - val_loss: 0.4169 - val_accuracy: 0.9230

Trial 4 Complete [01h 38m 49s]
val_loss: 0.4168871343135834

Best val_loss So Far: 0.2570400834083557
Total elapsed time: 06h 57m 59s

🎯 Hiperparámetros óptimos V11 CORREGIDOS encontrados:
   - Learning rate: 2e-06 (MÁS BAJA)
   - Dropout rate: 0.7 (MÁS AGRESIVO)
   - L2 regularization: 0.05 (MÁS FUERTE)
   - Noise factor: 0.03 (NUEVO - reemplaza label_smoothing)
   - Batch size óptimo: 4 (variable 4-8)
--- Fase 1: Cargando y procesando el corpus (MODO V11 - REGULARIZACIÓN MÁXIMA CORREGIDA) ---

--- Análisis de Balance del Corpus ---
Total de registros limpios: 61674
Noticias FALSAS (0): 30734 registros (49.8%)
Noticias REALES (1): 30940 registros (50.2%)
-------------------------------------------------

🔄 División V11: 70% entrenamiento, 10% validación, 20% pruebas...
Usando batch_size: 4
Tamaño del set de entrenamiento: 43171 registros (70.0%)
Tamaño del set de validación: 6167 registros (10.0%)
Tamaño del set de pruebas: 12336 registros (20.0%)

Cargando tokenizador para el modelo: 'distilbert-base-multilingual-cased'
C:\Users\gabri\.conda\envs\tf-env\lib\site-packages\huggingface_hub\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Tokenizando con MAX_LENGTH=128 (reducido para evitar overfitting)...

✅ Configuración V11 CORREGIDA guardada en: 'configuracion_experimento_antioverfit_v11.txt'

--- Fase 3: Entrenamiento Final V11 CORREGIDO ---
🎯 OBJETIVO: Gap de pérdida < 0.04
⚖️ Paciencia: 8 épocas
🛡️ Regularización MÁXIMA activada (sin label_smoothing)
Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ V11 CORREGIDA: Usando Adam con regularización MÁXIMA (sin label_smoothing)
✅ Weight decay manual 2X activado
✅ Noise injection activado (reemplaza label_smoothing)

🚀 Iniciando entrenamiento V11 CORREGIDO...
📊 EXPECTATIVA V11:
   • Gap de pérdida: < 0.04 (vs V10: ~0.10)
   • Convergencia mejorada: Líneas MÁS CERCA
   • Trade-off accuracy/generalización
============================================================
Epoch 1/30
10793/10793 [==============================] - ETA: 0s - loss: 2.1405 - accuracy: 0.5657
📊 V11 Monitoreo Época 1:
   Gap de pérdida (val - train): -1.4463
   Gap de exactitud (train - val): -0.1025
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_01.png
10793/10793 [==============================] - 1521s 140ms/step - loss: 2.1405 - accuracy: 0.5657 - val_loss: 0.6942 - val_accuracy: 0.6682 - lr: 2.0000e-06
Epoch 2/30
10793/10793 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7744
📊 V11 Monitoreo Época 2:
   Gap de pérdida (val - train): -0.1661
   Gap de exactitud (train - val): -0.1012
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_02.png
10793/10793 [==============================] - 1502s 139ms/step - loss: 0.5933 - accuracy: 0.7744 - val_loss: 0.4272 - val_accuracy: 0.8756 - lr: 2.0000e-06
Epoch 3/30
10793/10793 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.9001
📊 V11 Monitoreo Época 3:
   Gap de pérdida (val - train): -0.0590
   Gap de exactitud (train - val): -0.0153
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_03.png
10793/10793 [==============================] - 1503s 139ms/step - loss: 0.3898 - accuracy: 0.9001 - val_loss: 0.3308 - val_accuracy: 0.9154 - lr: 2.0000e-06
Epoch 4/30
10793/10793 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.9254
📊 V11 Monitoreo Época 4:
   Gap de pérdida (val - train): -0.0305
   Gap de exactitud (train - val): -0.0052
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_04.png
10793/10793 [==============================] - 1505s 139ms/step - loss: 0.3185 - accuracy: 0.9254 - val_loss: 0.2881 - val_accuracy: 0.9306 - lr: 2.0000e-06
Epoch 5/30
10793/10793 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9389
📊 V11 Monitoreo Época 5:
   Gap de pérdida (val - train): -0.0160
   Gap de exactitud (train - val): 0.0000
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_05.png
10793/10793 [==============================] - 1503s 139ms/step - loss: 0.2785 - accuracy: 0.9389 - val_loss: 0.2625 - val_accuracy: 0.9389 - lr: 2.0000e-06
Epoch 6/30
10793/10793 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9482
📊 V11 Monitoreo Época 6:
   Gap de pérdida (val - train): -0.0043
   Gap de exactitud (train - val): 0.0038
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_06.png
10793/10793 [==============================] - 1502s 139ms/step - loss: 0.2522 - accuracy: 0.9482 - val_loss: 0.2479 - val_accuracy: 0.9444 - lr: 2.0000e-06
Epoch 7/30
10793/10793 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.9546
📊 V11 Monitoreo Época 7:
   Gap de pérdida (val - train): 0.0127
   Gap de exactitud (train - val): 0.0134
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_07.png
10793/10793 [==============================] - 1505s 139ms/step - loss: 0.2342 - accuracy: 0.9546 - val_loss: 0.2469 - val_accuracy: 0.9411 - lr: 2.0000e-06
Epoch 8/30
10793/10793 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9607
📊 V11 Monitoreo Época 8:
   Gap de pérdida (val - train): 0.0184
   Gap de exactitud (train - val): 0.0116
🎉 V11 EXCELENTE: Gap < 0.02 (convergencia perfecta)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 127881 (\N{PARTY POPPER}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_08.png
10793/10793 [==============================] - 1507s 140ms/step - loss: 0.2175 - accuracy: 0.9607 - val_loss: 0.2359 - val_accuracy: 0.9491 - lr: 2.0000e-06
Epoch 9/30
10793/10793 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9649
📊 V11 Monitoreo Época 9:
   Gap de pérdida (val - train): 0.0214
   Gap de exactitud (train - val): 0.0162
✅ V11 BUENO: Gap entre 0.02-0.04 (objetivo alcanzado)
🎉 V11 EXCELENTE: Gap exactitud < 0.02
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9989 (\N{WHITE HEAVY CHECK MARK}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9989 (\N{WHITE HEAVY CHECK MARK}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_09.png
10793/10793 [==============================] - 1505s 139ms/step - loss: 0.2054 - accuracy: 0.9649 - val_loss: 0.2268 - val_accuracy: 0.9488 - lr: 2.0000e-06
Epoch 10/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9697
Epoch 10: ReduceLROnPlateau reducing learning rate to 2.999999992425728e-07.

📊 V11 Monitoreo Época 10:
   Gap de pérdida (val - train): 0.0571
   Gap de exactitud (train - val): 0.0302
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_10.png
10793/10793 [==============================] - 1504s 139ms/step - loss: 0.1925 - accuracy: 0.9697 - val_loss: 0.2496 - val_accuracy: 0.9395 - lr: 2.0000e-06
Epoch 11/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9744
📊 V11 Monitoreo Época 11:
   Gap de pérdida (val - train): 0.0442
   Gap de exactitud (train - val): 0.0240
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_11.png
10793/10793 [==============================] - 1512s 140ms/step - loss: 0.1777 - accuracy: 0.9744 - val_loss: 0.2220 - val_accuracy: 0.9504 - lr: 3.0000e-07
Epoch 12/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9755
Epoch 12: ReduceLROnPlateau reducing learning rate to 4.500000159168849e-08.

📊 V11 Monitoreo Época 12:
   Gap de pérdida (val - train): 0.0481
   Gap de exactitud (train - val): 0.0250
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_12.png
10793/10793 [==============================] - 1503s 139ms/step - loss: 0.1742 - accuracy: 0.9755 - val_loss: 0.2222 - val_accuracy: 0.9505 - lr: 3.0000e-07
Epoch 13/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9766
📊 V11 Monitoreo Época 13:
   Gap de pérdida (val - train): 0.0492
   Gap de exactitud (train - val): 0.0262
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_13.png
10793/10793 [==============================] - 1502s 139ms/step - loss: 0.1722 - accuracy: 0.9766 - val_loss: 0.2214 - val_accuracy: 0.9504 - lr: 4.5000e-08
Epoch 14/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9767
Epoch 14: ReduceLROnPlateau reducing learning rate to 6.750000025590452e-09.

📊 V11 Monitoreo Época 14:
   Gap de pérdida (val - train): 0.0503
   Gap de exactitud (train - val): 0.0262
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_14.png
10793/10793 [==============================] - 1500s 139ms/step - loss: 0.1723 - accuracy: 0.9767 - val_loss: 0.2226 - val_accuracy: 0.9505 - lr: 4.5000e-08
Epoch 15/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9765
Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0125000171612442e-09.

📊 V11 Monitoreo Época 15:
   Gap de pérdida (val - train): 0.0496
   Gap de exactitud (train - val): 0.0260
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_15.png
10793/10793 [==============================] - 2609s 242ms/step - loss: 0.1722 - accuracy: 0.9765 - val_loss: 0.2218 - val_accuracy: 0.9505 - lr: 6.7500e-09
Epoch 16/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9765
Epoch 16: ReduceLROnPlateau reducing learning rate to 1e-09.

📊 V11 Monitoreo Época 16:
   Gap de pérdida (val - train): 0.0497
   Gap de exactitud (train - val): 0.0262
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_16.png
10793/10793 [==============================] - 4241s 393ms/step - loss: 0.1720 - accuracy: 0.9765 - val_loss: 0.2217 - val_accuracy: 0.9504 - lr: 1.0125e-09
Epoch 17/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9766
📊 V11 Monitoreo Época 17:
   Gap de pérdida (val - train): 0.0499
   Gap de exactitud (train - val): 0.0262
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_17.png
10793/10793 [==============================] - 1515s 140ms/step - loss: 0.1717 - accuracy: 0.9766 - val_loss: 0.2216 - val_accuracy: 0.9504 - lr: 1.0000e-09
Epoch 18/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9776
📊 V11 Monitoreo Época 18:
   Gap de pérdida (val - train): 0.0506
   Gap de exactitud (train - val): 0.0272
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_18.png
10793/10793 [==============================] - 1511s 140ms/step - loss: 0.1709 - accuracy: 0.9776 - val_loss: 0.2216 - val_accuracy: 0.9504 - lr: 1.0000e-09
Epoch 19/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9766
📊 V11 Monitoreo Época 19:
   Gap de pérdida (val - train): 0.0502
   Gap de exactitud (train - val): 0.0262
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_19.png
10793/10793 [==============================] - 1481s 137ms/step - loss: 0.1713 - accuracy: 0.9766 - val_loss: 0.2215 - val_accuracy: 0.9504 - lr: 1.0000e-09
Epoch 20/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9765
📊 V11 Monitoreo Época 20:
   Gap de pérdida (val - train): 0.0499
   Gap de exactitud (train - val): 0.0261
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_20.png
10793/10793 [==============================] - 1490s 138ms/step - loss: 0.1717 - accuracy: 0.9765 - val_loss: 0.2215 - val_accuracy: 0.9504 - lr: 1.0000e-09
Epoch 21/30
10793/10793 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9772Restoring model weights from the end of the best epoch: 13.

📊 V11 Monitoreo Época 21:
   Gap de pérdida (val - train): 0.0504
   Gap de exactitud (train - val): 0.0268
⚠️ V11 ALERTA: Gap > 0.04 (fuera del objetivo)
✅ V11 BUENO: Gap exactitud entre 0.02-0.04
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:321: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:322: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig(f'curva_convergencia_v11_epoca_{self.epochs[-1]:02d}.png', dpi=300, bbox_inches='tight')
📊 Gráfica V11 guardada: curva_convergencia_v11_epoca_21.png
10793/10793 [==============================] - 1507s 140ms/step - loss: 0.1711 - accuracy: 0.9772 - val_loss: 0.2215 - val_accuracy: 0.9504 - lr: 1.0000e-09
Epoch 21: early stopping
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 127919 (\N{DIRECT HIT}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 128202 (\N{BAR CHART}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 11088 (\N{WHITE MEDIUM STAR}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 9989 (\N{WHITE HEAVY CHECK MARK}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 128200 (\N{CHART WITH UPWARDS TREND}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 9878 (\N{SCALES}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:468: UserWarning: Glyph 128201 (\N{CHART WITH DOWNWARDS TREND}) missing from current font.
  plt.tight_layout()
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 127919 (\N{DIRECT HIT}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 9888 (\N{WARNING SIGN}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 65039 (\N{VARIATION SELECTOR-16}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 128202 (\N{BAR CHART}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 11088 (\N{WHITE MEDIUM STAR}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 9989 (\N{WHITE HEAVY CHECK MARK}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 128200 (\N{CHART WITH UPWARDS TREND}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 9878 (\N{SCALES}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')
entrenar_distilbert_es_unificado_V11.py:469: UserWarning: Glyph 128201 (\N{CHART WITH DOWNWARDS TREND}) missing from current font.
  plt.savefig('analisis_convergencia_completo_v11.png', dpi=300, bbox_inches='tight')

✅ ANÁLISIS V11 COMPLETO: analisis_convergencia_completo_v11.png
🎯 Objetivo V11: Gap < 0.04 ❌ NO ALCANZADO
📊 Gap final: 0.050

✅ Entrenamiento V11 CORREGIDO completado: 21 épocas

📊 ANÁLISIS V11 CORREGIDO - OBJETIVO GAP < 0.04:
   🎯 Gap de pérdida final: 0.0504
   ⚠️ RESULTADO: MEJORADO pero no objetivo (< 0.07)
   📈 Mejora vs V10: 0.050 (49.6%)
   ⭐ Mejor época: 13
   📊 Épocas post-mejor: 8

--- Fase 4: Evaluación V11 CORREGIDA ---
3084/3084 [==============================] - 98s 31ms/step

🏆 RESULTADOS FINALES V11 CORREGIDA:
   • Exactitud: 0.952 (95.2%)
   • Gap objetivo: ❌ NO ALCANZADO
   • Convergencia: MEJORADA
   • Técnicas usadas: Weight decay 2X + Noise injection + Batch variable + L2 fuerte

--- Fase 5: Generación de Gráficas V11 ---
✅ Matriz de confusión V11 guardada como 'matriz_confusion_v11.png'

--- Fase 6: Guardando modelo V11 ---

================================================================================
🎉 EXPERIMENTO V11 CORREGIDO COMPLETADO
================================================================================
🎯 OBJETIVO V11: Gap < 0.04
📊 RESULTADO: Gap = 0.0504
🏆 ESTADO: ⚠️ PARCIAL
📈 MEJORA vs V10: 49.6%
🔧 Técnicas: Sin label_smoothing + Noise injection + Regulariación máxima
